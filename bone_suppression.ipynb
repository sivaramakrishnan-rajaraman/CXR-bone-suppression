  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#this is the graphical abstract of the proposed study\n",
    "from IPython.display import Image\n",
    "Image(\"striking_image.png\", width = 600, height = 600)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#get current working directory\n",
    "import os\n",
    "os.getcwd()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 1: Modality specific pretraining: \n",
    "The first step in repeated CXR_specific pretraining is to train the ImageNet-pretrained model on a large scale selection of CXRs to clasify them into normal and abnormal classes. Here, we retrained an ImageNet-pretrained VGG-16 model on a large, combined selection of CXRs including RSNA and pediatric pneumonia CXR data collections with sufficient diversity in terms of image acquisition and patient age to coarsely learn the characteristics of abnormal and normal lungs. This CXR modality-specific retraining helps in improving the specificity of the network weights conforming to the CXR classification task under study. This approach is followed toward learning CXR modality-specific characteristics about the normal lungs and an extensive selection of pulmonary abnormalities. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "#%% load libraries; clear current gpu session\n",
    "import warnings \n",
    "warnings.filterwarnings('ignore',category=FutureWarning) #because of numpy version\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras import backend as K\n",
    "K.clear_session()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "from keras.initializers import he_normal\n",
    "from keras import applications, activations\n",
    "from keras.applications.vgg16 import VGG16\n",
    "from keras.models import Sequential, Model, Input, load_model\n",
    "from keras.layers import Conv2D, Dense, MaxPooling2D, SeparableConv2D, BatchNormalization, ZeroPadding2D, GlobalAveragePooling2D,Flatten,Average, Dropout\n",
    "from keras.layers import Add, Activation, Dropout, Flatten, Dense, Lambda, LeakyReLU, PReLU\n",
    "from keras.applications import VGG16\n",
    "from vis.utils import utils\n",
    "from matplotlib import pyplot as plt\n",
    "from vis.visualization import visualize_saliency, overlay\n",
    "import matplotlib.cm as cm\n",
    "import pandas as pd\n",
    "import cv2\n",
    "import numpy as np\n",
    "import itertools\n",
    "import glob\n",
    "import struct\n",
    "import zlib\n",
    "import imutils\n",
    "import pickle\n",
    "import shutil\n",
    "import statistics\n",
    "from math import pi\n",
    "from math import cos\n",
    "from math import floor\n",
    "from keras import backend\n",
    "import statistics\n",
    "from scipy import interp\n",
    "from skimage.segmentation import mark_boundaries\n",
    "from sklearn.metrics import precision_recall_curve, average_precision_score, matthews_corrcoef, mean_squared_error, mean_squared_log_error\n",
    "from sklearn.metrics import classification_report,confusion_matrix, roc_curve, auc, accuracy_score, log_loss\n",
    "from keras.callbacks import ModelCheckpoint, TensorBoard, ReduceLROnPlateau, EarlyStopping, CSVLogger\n",
    "import scikitplot as skplt\n",
    "from itertools import cycle\n",
    "from sklearn.utils import class_weight\n",
    "from keras.utils import plot_model, to_categorical\n",
    "from keras.preprocessing import image\n",
    "from keras.preprocessing.image import ImageDataGenerator\n",
    "from keras.optimizers import Adam, SGD\n",
    "from sklearn.preprocessing import label_binarize\n",
    "from scipy.ndimage.interpolation import zoom\n",
    "from keras import backend as K\n",
    "from keras.preprocessing.image import load_img, img_to_array\n",
    "from sklearn import metrics\n",
    "from scipy.optimize import minimize\n",
    "import time\n",
    "import pandas as pd\n",
    "from sklearn.metrics import confusion_matrix\n",
    "from keras.preprocessing.image import img_to_array\n",
    "from keras.optimizers import Adam, SGD\n",
    "from sklearn.metrics import precision_recall_curve\n",
    "from sklearn.metrics import average_precision_score\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "from itertools import cycle\n",
    "from sklearn.utils import class_weight\n",
    "from tqdm import tqdm\n",
    "from sklearn.model_selection import train_test_split\n",
    "from PIL import Image\n",
    "import imageio\n",
    "from glob import glob\n",
    "import skimage.io as io"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tensor(\"input_1:0\", shape=(?, 256, 256, 3), dtype=float32)\n"
     ]
    }
   ],
   "source": [
    "#%% Loading the combined training data split at the patient level at 90% for training and 10% for testing\n",
    "# we allocated 10% of the training data for validation with a fixed seed.\n",
    "\n",
    "img_width, img_height = 256,256\n",
    "train_data_dir = 'C:/Users/codes/train/'\n",
    "test_data_dir = 'C:/Users/codes/test/'\n",
    "epochs = 32\n",
    "batch_size = 16\n",
    "num_classes = 2 #abnormal, normal\n",
    "input_shape = (img_width, img_height, 3)\n",
    "model_input = Input(shape=input_shape)\n",
    "print(model_input)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#define data generators; https://github.com/keras-team/keras/issues/5862\n",
    "\n",
    "train_datagen = ImageDataGenerator(\n",
    "        rescale=1./255,\n",
    "        validation_split=0.1,\n",
    "        rotation_range=5,\n",
    "        width_shift_range=0.2,\n",
    "        height_shift_range=0.2,\n",
    "        horizontal_flip=True,\n",
    "        fill_mode='nearest') #10% split\n",
    "\n",
    "val_datagen = ImageDataGenerator(\n",
    "        rescale=1./255, \n",
    "        validation_split=0.1)  \n",
    "\n",
    "test_datagen = ImageDataGenerator(\n",
    "        rescale=1./255) \n",
    "\n",
    "train_generator = train_datagen.flow_from_directory(\n",
    "    train_data_dir,\n",
    "    seed = 13,\n",
    "    target_size=(img_height, img_width),\n",
    "    batch_size=batch_size,\n",
    "    shuffle=True,\n",
    "    class_mode='categorical',\n",
    "    subset='training') # set as training data\n",
    "\n",
    "validation_generator = val_datagen.flow_from_directory(\n",
    "    train_data_dir, \n",
    "    seed = 13,\n",
    "    target_size=(img_height, img_width),\n",
    "    batch_size=batch_size,\n",
    "    shuffle=True,\n",
    "    class_mode='categorical',\n",
    "    subset='validation') \n",
    "\n",
    "test_generator = test_datagen.flow_from_directory(\n",
    "        test_data_dir,\n",
    "        target_size=(img_width, img_height),\n",
    "        batch_size=batch_size, \n",
    "        class_mode='categorical', \n",
    "        shuffle = False)\n",
    "\n",
    "#identify the number of samples\n",
    "nb_train_samples = len(train_generator.filenames)\n",
    "nb_validation_samples = len(validation_generator.filenames)\n",
    "nb_test_samples = len(test_generator.filenames)\n",
    "\n",
    "#check the class indices\n",
    "print(train_generator.class_indices)\n",
    "print(validation_generator.class_indices)\n",
    "print(test_generator.class_indices)\n",
    "\n",
    "#true labels\n",
    "Y_test=test_generator.classes\n",
    "print(Y_test.shape)\n",
    "\n",
    "#convert test labels to categorical\n",
    "Y_test1=to_categorical(Y_test, num_classes=num_classes, dtype='float32')\n",
    "print(Y_test1.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#compute class weights to penalize over represented classes\n",
    "\n",
    "class_weights = class_weight.compute_class_weight(\n",
    "               'balanced',\n",
    "                np.unique(train_generator.classes), \n",
    "                train_generator.classes)\n",
    "print(class_weights)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Instantiate the ImageNet-pretrained VGG-16 model, \n",
    "#truncate at the deepest convolutional layer and append GAP, dropout and final dense layer with two neurons\n",
    "\n",
    "vgg16_cnn = VGG16(include_top=False, weights='imagenet', \n",
    "                        input_tensor=model_input)\n",
    "vgg16_cnn.summary()\n",
    "base_model_vgg16=Model(inputs=vgg16_cnn.input,\n",
    "                        outputs=vgg16_cnn.get_layer('block5_conv3').output)\n",
    "x = base_model_vgg16.output    \n",
    "x = GlobalAveragePooling2D()(x)\n",
    "x = Dropout(0.5)(x)\n",
    "predictions = Dense(num_classes, \n",
    "                    activation='softmax', name='predictions')(x)\n",
    "vgg16_model = Model(inputs=base_model_vgg16.input, \n",
    "                    outputs=predictions, \n",
    "                    name = 'vgg16_cxrpretrained')\n",
    "vgg16_model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#%% compile and train the model\n",
    "sgd = SGD(lr=0.001, decay=1e-6, momentum=0.9, nesterov=True)  \n",
    "vgg16_model.compile(optimizer=sgd, loss='categorical_crossentropy', metrics=['accuracy']) \n",
    "\n",
    "filepath = 'weights1/' + vgg16_model.name + '.{epoch:02d}-{val_acc:.4f}.h5'\n",
    "checkpoint = ModelCheckpoint(filepath, monitor='val_loss', verbose=1, \n",
    "                             save_weights_only=False, save_best_only=True, \n",
    "                             mode='min', period=1)\n",
    "earlyStopping = EarlyStopping(monitor='val_loss', \n",
    "                               patience=5, verbose=1, mode='min')\n",
    "tensor_board = TensorBoard(log_dir='logs/', histogram_freq=0, batch_size=batch_size)\n",
    "reduce_lr = ReduceLROnPlateau(monitor='val_loss', factor=0.5, patience=5,\n",
    "                              verbose=1, mode='min', min_lr=0.00001)\n",
    "callbacks_list = [checkpoint, tensor_board, earlyStopping, reduce_lr]\n",
    "\n",
    "#reset generators\n",
    "train_generator.reset()\n",
    "validation_generator.reset()\n",
    "\n",
    "#train the model\n",
    "history = vgg16_model.fit_generator(train_generator, \n",
    "                                    steps_per_epoch=nb_train_samples // batch_size + 1,\n",
    "                                    epochs=epochs, \n",
    "                                    validation_data=validation_generator,\n",
    "                                      class_weight = class_weights,\n",
    "                                      callbacks=callbacks_list, \n",
    "                                      validation_steps=nb_validation_samples // batch_size + 1, \n",
    "                                      verbose=1) \n",
    "#see if the steps per epochs and validation steps are absolutely divisble, otherwise add 1\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#predict using the trained model\n",
    "\n",
    "vgg16_model.load_weights('weights1/vgg16_cxrpretrained.h5') #path to your model\n",
    "vgg16_model.summary()\n",
    "\n",
    "#compile\n",
    "sgd = SGD(lr=0.001, decay=1e-6, momentum=0.9, nesterov=True)  \n",
    "vgg16_model.compile(optimizer=sgd, loss='categorical_crossentropy', \n",
    "                     metrics=['accuracy']) \n",
    "\n",
    "\n",
    "#measure performance on test data, first reset the test generator otherwise it gives wierd results\n",
    "test_generator.reset()\n",
    "\n",
    "#evaluate accuracy \n",
    "custom_y_pred = vgg16_model.predict_generator(test_generator,\n",
    "                                              nb_test_samples // batch_size + 1, verbose=1) \n",
    "#if not absolutely divisible, otherwise remove 1\n",
    "\n",
    "custom_y_pred1 = custom_y_pred.argmax(axis=-1)\n",
    "\n",
    "save the predictions\n",
    "np.savetxt('weights1/vgg16_y_pred_finetuned.csv', custom_y_pred1,fmt='%f',delimiter = \",\")\n",
    "np.savetxt('weights1/Y_test.csv',Y_test,fmt='%i',delimiter = \",\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#measure performance\n",
    "accuracy = accuracy_score(Y_test1.argmax(axis=-1),\n",
    "                          custom_y_pred.argmax(axis=-1))\n",
    "print('The test accuracy of the Custom model is: ', accuracy)\n",
    "\n",
    "prec = precision_score(Y_test,custom_y_pred.argmax(axis=-1), \n",
    "                       average='weighted') #options: macro, weighted\n",
    "print('The precision of the Custom model is: ', prec)\n",
    "\n",
    "rec = recall_score(Y_test,custom_y_pred.argmax(axis=-1), \n",
    "                   average='weighted')\n",
    "print('The recall of the Custom model is: ', rec)\n",
    "\n",
    "f1 = f1_score(Y_test,custom_y_pred.argmax(axis=-1), \n",
    "              average='weighted')\n",
    "print('The f1-score of the Custom model is: ', f1)\n",
    "\n",
    "mat_coeff = matthews_corrcoef(Y_test,custom_y_pred.argmax(axis=-1))\n",
    "print('The MCC of the Custom model is: ', mat_coeff)\n",
    "\n",
    "kappa = cohen_kappa_score(Y_test,\n",
    "                          custom_y_pred.argmax(axis=-1))\n",
    "print('The cohen kappa score of the Custom model is: ', kappa)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#plot confusion matrix\n",
    "\n",
    "def plot_confusion_matrix(cm, classes,\n",
    "                          normalize=False,\n",
    "                          title='Confusion matrix',\n",
    "                          cmap=plt.cm.Greys):\n",
    "    plt.imshow(cm, interpolation='nearest', cmap=cmap)\n",
    "    title_font = {'fontname':'Arial', 'size':'30', 'color':'black', \n",
    "                  'weight':'normal', 'verticalalignment':'bottom'} \n",
    "    axis_font = {'fontname':'Arial', 'size':'20'}\n",
    "    #plt.title(title, **title_font)\n",
    "    plt.colorbar()\n",
    "    tick_marks = np.arange(len(classes))\n",
    "    plt.xticks(tick_marks, classes, fontsize=20, rotation=0)\n",
    "    plt.yticks(tick_marks, classes, fontsize=20, rotation=0)\n",
    "\n",
    "    if normalize:\n",
    "        cm = cm.astype('float') / cm.sum(axis=1)[:, np.newaxis]\n",
    "        print(\"Normalized confusion matrix\")\n",
    "    else:\n",
    "        print('Confusion matrix, without normalization')\n",
    "    print(cm)\n",
    "    thresh = cm.max() / 2.\n",
    "    for i, j in itertools.product(range(cm.shape[0]), range(cm.shape[1])):\n",
    "        plt.text(j, i, cm[i, j], fontsize=30,\n",
    "                 horizontalalignment=\"center\",\n",
    "                 color=\"white\" if cm[i, j] > thresh else \"black\")\n",
    "    plt.tight_layout()\n",
    "    plt.ylabel('True label', **axis_font)\n",
    "    plt.xlabel('Predicted label', **axis_font)   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#%% print classification report and plot confusion matrix\n",
    "\n",
    "import itertools\n",
    "target_names = ['Abnormal','Normal'] \n",
    "print(classification_report(Y_test1.argmax(axis=-1),\n",
    "                            custom_y_pred.argmax(axis=-1),\n",
    "                            target_names=target_names, digits=4))\n",
    "\n",
    "# Compute confusion matrix\n",
    "cnf_matrix = confusion_matrix(Y_test1.argmax(axis=-1),\n",
    "                              custom_y_pred.argmax(axis=-1))\n",
    "np.set_printoptions(precision=4)\n",
    "\n",
    "# Plot non-normalized confusion matrix using scikit learn\n",
    "plt.figure(figsize=(10,10), dpi=600)\n",
    "plot_confusion_matrix(cnf_matrix, classes=target_names)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#%% Plot AUC curves\n",
    "class_ = ['Abnormal','Normal']  \n",
    "num_classes = len(class_)\n",
    "\n",
    "# Compute ROC curve and ROC area for each class\n",
    "lw = 2\n",
    "fpr = dict()\n",
    "tpr = dict()\n",
    "roc_auc = dict()\n",
    "for i in range(num_classes):\n",
    "    fpr[i], tpr[i], _ = roc_curve(Y_test1[:, i], custom_y_pred[:, i])\n",
    "    roc_auc[i] = auc(fpr[i], tpr[i])\n",
    "  \n",
    "# Compute micro-average ROC curve and ROC area\n",
    "fpr[\"micro\"], tpr[\"micro\"], _ = roc_curve(Y_test1.ravel(), \n",
    "                                          custom_y_pred.ravel())\n",
    "roc_auc[\"micro\"] = auc(fpr[\"micro\"], tpr[\"micro\"])\n",
    "\n",
    "# First aggregate all false positive rates\n",
    "all_fpr = np.unique(np.concatenate([fpr[i] for i in range(num_classes)]))\n",
    "\n",
    "# Then interpolate all ROC curves at this points\n",
    "mean_tpr = np.zeros_like(all_fpr)\n",
    "for i in range(num_classes):\n",
    "    mean_tpr += np.interp(all_fpr, fpr[i], tpr[i])\n",
    "\n",
    "# Finally average it and compute AUC\n",
    "mean_tpr /= num_classes\n",
    "\n",
    "fpr[\"macro\"] = all_fpr\n",
    "tpr[\"macro\"] = mean_tpr\n",
    "roc_auc[\"macro\"] = auc(fpr[\"macro\"], tpr[\"macro\"])\n",
    "\n",
    "# Plot all ROC curves\n",
    "fig=plt.figure(figsize=(15,10), dpi=600)\n",
    "ax = fig.add_subplot(1, 1, 1)\n",
    "ax.set_facecolor('white')\n",
    "major_ticks = np.arange(0.0, 1.1, 0.20) \n",
    "minor_ticks = np.arange(0.0, 1.1, 0.20)\n",
    "ax.set_xticks(major_ticks)\n",
    "ax.set_xticks(minor_ticks, minor=True)\n",
    "ax.set_yticks(major_ticks)\n",
    "ax.set_yticks(minor_ticks, minor=True)\n",
    "plt.plot([0, 1], [0, 1], 'k--', lw=lw)\n",
    "colors = cycle(['red', 'blue', 'indigo'])\n",
    "for i, color in zip(range(num_classes), colors):\n",
    "    plt.plot(fpr[i], tpr[i], color=color, lw=3, \n",
    "             label='{0} class (AUC = {1:0.4f})'\n",
    "             .format(class_[i], roc_auc[i]))\n",
    "plt.plot(fpr[\"micro\"], tpr[\"micro\"],\n",
    "          label='micro-average (AUC = {0:0.4f})'\n",
    "                ''.format(roc_auc[\"micro\"]),\n",
    "          color='green', linestyle='solid', linewidth=4) # 'deeppink'\n",
    "\n",
    "plt.xticks(fontsize=20)\n",
    "plt.yticks(fontsize=20)\n",
    "plt.xlabel('False Positive Rate', fontsize=20)\n",
    "plt.ylabel('True Positive Rate', fontsize=20)\n",
    "plt.legend(loc=\"lower right\", prop={\"size\":20})\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#%% compute precision-recall curves\n",
    "\n",
    "colors = cycle(['red', 'blue', 'green', 'cyan', 'teal'])\n",
    "class_ = ['Abnormal','Normal']  \n",
    "num_classes = len(class_)\n",
    "\n",
    "plt.figure(figsize=(15,10), dpi=600)\n",
    "f_scores = np.linspace(0.2, 0.8, num=4)\n",
    "lines = []\n",
    "labels = []\n",
    "for f_score in f_scores:\n",
    "    x = np.linspace(0.01, 1)\n",
    "    y = f_score * x / (2 * x - f_score)\n",
    "    l, = plt.plot(x[y >= 0], y[y >= 0], color='gray', alpha=0.2)\n",
    "    plt.annotate('f1={0:0.1f}'.format(f_score), xy=(0.9, y[45] + 0.02))\n",
    "    \n",
    "# For each class\n",
    "precision = dict()\n",
    "recall = dict()\n",
    "average_precision = dict()\n",
    "for i in range(num_classes):\n",
    "    precision[i], recall[i], _ = precision_recall_curve(Y_test1[:, i],\n",
    "                                                        custom_y_pred[:, i])\n",
    "    average_precision[i] = average_precision_score(Y_test1[:, i], custom_y_pred[:, i])\n",
    "\n",
    "# A \"micro-average\": quantifying score on all classes jointly\n",
    "precision[\"micro\"], recall[\"micro\"], _ = precision_recall_curve(Y_test1.ravel(),\n",
    "   custom_y_pred.ravel())\n",
    "average_precision[\"micro\"] = average_precision_score(Y_test1, custom_y_pred,\n",
    "                                                     average=\"micro\")\n",
    "print('Average precision score, micro-averaged over all classes: {0:0.4f}'\n",
    "      .format(average_precision[\"micro\"]))\n",
    "\n",
    "lines.append(l)\n",
    "labels.append('iso-f1 curves')\n",
    "l, = plt.plot(recall[\"micro\"], precision[\"micro\"], color='green', lw=2)\n",
    "lines.append(l)\n",
    "labels.append('micro-average Precision-recall (area = {0:0.4f})'\n",
    "              ''.format(average_precision[\"micro\"]))\n",
    "\n",
    "for i, color in zip(range(num_classes), colors):\n",
    "    l, = plt.plot(recall[i], precision[i], color=color, lw=2)\n",
    "    lines.append(l)\n",
    "    labels.append('Precision-recall for class {0} (area = {1:0.4f})'\n",
    "                  ''.format(class_[i], average_precision[i]))\n",
    "\n",
    "fig = plt.gcf()\n",
    "fig.subplots_adjust(bottom=0.05)\n",
    "plt.xlim([0.0, 1.0])\n",
    "plt.ylim([0.0, 1.05])\n",
    "plt.xticks(fontsize=14, rotation=0)\n",
    "plt.yticks(fontsize=14, rotation=0)\n",
    "plt.xlabel('Recall', fontsize=20)\n",
    "plt.ylabel('Precision', fontsize=20)\n",
    "plt.title('PR curve')\n",
    "plt.legend(lines, labels, loc=\"lower left\", prop={\"size\":20})\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Now fine-tune the CXR pretrained VGG-16 model to classify the CXRs into TB and normal classes. Toward this classification task, a set of 20 radiographs (20% of the data) from the Montgomery TB dataset of which 10 CXRs showing normal lungs and the rest showing pulmonary TB manifestations is reserved as the hold-out test set. The remaining images (80% of the data) are split into four folds for performing cross-validation studies. Repeat the same process with the bone-suppressed counterpart."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "img_width, img_height = 256,256\n",
    "train_data_dir = \"../fine_tuning/train\"\n",
    "val_data_dir = \"../fine_tuning/val\"\n",
    "test_data_dir = \"../fine_tuning/test\"\n",
    "epochs = 32\n",
    "batch_size = 16\n",
    "num_classes = 2 # abnormal(TB), normal\n",
    "input_shape = (img_width, img_height, 3)\n",
    "model_input = Input(shape=input_shape)\n",
    "print(model_input)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#define data generators\n",
    "train_datagen = ImageDataGenerator(\n",
    "        rescale=1./255,\n",
    "        rotation_range=5,\n",
    "        width_shift_range=0.2,\n",
    "        height_shift_range=0.2,\n",
    "        horizontal_flip=True,\n",
    "        fill_mode='nearest') \n",
    "\n",
    "datagen = ImageDataGenerator(\n",
    "        rescale=1./255) \n",
    "\n",
    "train_generator = train_datagen.flow_from_directory(\n",
    "        train_data_dir,\n",
    "        target_size=(img_width, img_height),\n",
    "        batch_size=batch_size, \n",
    "        class_mode='categorical', \n",
    "        shuffle = True)\n",
    "\n",
    "val_generator = datagen.flow_from_directory(\n",
    "        val_data_dir,\n",
    "        target_size=(img_width, img_height),\n",
    "        batch_size=batch_size, \n",
    "        class_mode='categorical', \n",
    "        shuffle = False)\n",
    "\n",
    "test_generator = datagen.flow_from_directory(\n",
    "        test_data_dir,\n",
    "        target_size=(img_width, img_height),\n",
    "        batch_size=batch_size, \n",
    "        class_mode='categorical', \n",
    "        shuffle = False)\n",
    "\n",
    "#identify the number of samples\n",
    "nb_train_samples = len(train_generator.filenames)\n",
    "nb_val_samples = len(val_generator.filenames)\n",
    "nb_test_samples = len(test_generator.filenames)\n",
    "\n",
    "#check the class indices\n",
    "print(train_generator.class_indices)\n",
    "print(val_generator.class_indices)\n",
    "print(test_generator.class_indices)\n",
    "\n",
    "#true labels\n",
    "Y_test=test_generator.classes\n",
    "print(Y_test.shape)\n",
    "\n",
    "#convert test labels to categorical\n",
    "Y_test1=to_categorical(Y_test, num_classes=num_classes, dtype='float32')\n",
    "print(Y_test1.shape)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "initialize the model from the CXR-pretraining stage\n",
    "The model is truncated at its deepest convolutional layer \n",
    "and appended with (i) GAP layer, (ii) dropout layer (ratio = 0.5), and \n",
    "(iii) dense layer with Softmax activation to output class probabilities\n",
    "for TB and normal CXRs\n",
    "'''\n",
    "vgg16_cnn = load_model('weights/vgg16_cxrpretrained.10-0.9514.h5') #path to your model \n",
    "vgg16_cnn.summary()\n",
    "base_model_vgg16=Model(inputs=vgg16_cnn.input,\n",
    "                        outputs=vgg16_cnn.get_layer('block5_conv3').output)\n",
    "x = base_model_vgg16.output    \n",
    "x = GlobalAveragePooling2D()(x)\n",
    "x = Dropout(0.5)(x)\n",
    "predictions = Dense(num_classes, \n",
    "                    activation='softmax', name='predictions')(x)\n",
    "model_vgg16 = Model(inputs=base_model_vgg16.input, \n",
    "                    outputs=predictions, \n",
    "                    name = 'vgg16_finetuned')\n",
    "model_vgg16.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#%% compile and train the models\n",
    "sgd = SGD(lr=0.001, decay=1e-6, momentum=0.9, nesterov=True)  \n",
    "model_vgg16.compile(optimizer=sgd, loss='categorical_crossentropy', metrics=['accuracy']) \n",
    "\n",
    "filepath = 'weights1/' + model_vgg16.name + '.{epoch:02d}-{val_acc:.4f}.h5'\n",
    "checkpoint = ModelCheckpoint(filepath, monitor='val_loss', verbose=1, \n",
    "                             save_weights_only=False, save_best_only=True, \n",
    "                             mode='min', period=1)\n",
    "earlyStopping = EarlyStopping(monitor='val_loss', \n",
    "                               patience=5, verbose=1, mode='min')\n",
    "tensor_board = TensorBoard(log_dir='logs/', histogram_freq=0, batch_size=batch_size)\n",
    "reduce_lr = ReduceLROnPlateau(monitor='val_loss', factor=0.5, patience=10,\n",
    "                              verbose=1, mode='min', min_lr=0.00001)\n",
    "callbacks_list = [checkpoint, tensor_board, earlyStopping, reduce_lr]\n",
    "\n",
    "#reset generators\n",
    "train_generator.reset()\n",
    "val_generator.reset()\n",
    "\n",
    "#train the model\n",
    "start = time.time()\n",
    "history = model_vgg16.fit_generator(train_generator, \n",
    "                                    steps_per_epoch=nb_train_samples // batch_size + 1,\n",
    "                                    epochs=epochs, \n",
    "                                    validation_data=val_generator,\n",
    "                                    class_weight = class_weights,\n",
    "                                    callbacks=callbacks_list, \n",
    "                                    validation_steps=nb_val_samples // batch_size + 1, \n",
    "                                    verbose=1) \n",
    "#see if the steps per epochs and validation steps are absolutely divisble, otherwise add 1 to each\n",
    "\n",
    "#print the total time taken for training\n",
    "print(time.time()-start)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#predict using the test data\n",
    "\n",
    "model_vgg16.load_weights('weights1/vgg16_finetuned.h5') #path to your model\n",
    "model_vgg16.summary()\n",
    "\n",
    "#compile\n",
    "sgd = SGD(lr=0.001, decay=1e-6, momentum=0.9, nesterov=True)  \n",
    "model_vgg16.compile(optimizer=sgd, loss='categorical_crossentropy', \n",
    "                     metrics=['accuracy']) \n",
    "\n",
    "#measure performance on test data, first reset the test generator otherwise it gives wierd results\n",
    "test_generator.reset()\n",
    "\n",
    "#evaluate accuracy \n",
    "custom_y_pred = model_vgg16.predict_generator(test_generator,\n",
    "                                              nb_test_samples // batch_size + 1, verbose=1) \n",
    "custom_y_pred1 = custom_y_pred.argmax(axis=-1)\n",
    "\n",
    "#save the predictions\n",
    "np.savetxt('weights1/vgg16_y_pred_finetuned.csv',\n",
    "             custom_y_pred1,fmt='%f',delimiter = \",\")\n",
    "np.savetxt('weights1/Y_test.csv',Y_test,fmt='%i',delimiter = \",\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Measure performance metrics, AUC curves and Precision recall curves as before"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Perform Class-selective relevance mapping using the fine-tuned model:\n",
    "We used CRMs to interpret the predictions of the best-performing baseline and bone-suppressed models using the hold-out test set to localize TB-consistent findings. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#this is the graphical abstract of the proposed study\n",
    "from IPython.display import Image\n",
    "Image(\"crm.png\", width = 600, height = 600)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#custom function to enhance image resolution\n",
    "def writePNGwithdpi(im, filename, dpi=(300,300)):\n",
    "    retval, buffer = cv2.imencode(\".png\", im)\n",
    "    s = buffer.tostring()\n",
    "    IDAToffset = s.find(b'IDAT') - 4\n",
    "    pHYs = b'pHYs' + struct.pack('!IIc',int(dpi[0]/0.0254),int(dpi[1]/0.0254),b\"\\x01\" ) \n",
    "    pHYs = struct.pack('!I',9) + pHYs + struct.pack('!I',zlib.crc32(pHYs))\n",
    "    with open(filename, \"wb\") as out:\n",
    "        out.write(buffer[0:IDAToffset])\n",
    "        out.write(pHYs)\n",
    "        out.write(buffer[IDAToffset:])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "img_width, img_height = 256, 256\n",
    "\n",
    "#load model\n",
    "vgg16_custom_model = load_model('weights1/vgg16_finetuned.h5')\n",
    "vgg16_custom_model.summary()\n",
    "\n",
    "#compile the model\n",
    "sgd = SGD(lr=1e-3, decay=1e-6, momentum=0.9, nesterov=True) #optimize to your requirements\n",
    "vgg16_custom_model.compile(optimizer=sgd,\n",
    "              loss='categorical_crossentropy',\n",
    "              metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#path to image to visualize\n",
    "from keras.preprocessing import image\n",
    "img_path = 'patient3_1.png' #path to image\n",
    "img = image.load_img(img_path)\n",
    "\n",
    "#preprocess the image\n",
    "x = image.img_to_array(img)\n",
    "x = np.expand_dims(x, axis=0)\n",
    "x /= 255 \n",
    "\n",
    "#predict on the image\n",
    "preds = vgg16_custom_model.predict(x)[0]\n",
    "print(preds)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Define CRM visualization for the classification task:\n",
    "\n",
    "def Generate_CRM_2Class(thisModel, thisImg_path, Threshold):       # generate Class Revlevance Map (CRM)\n",
    "    try:\n",
    "        # preprocess input image      \n",
    "        width, height = thisModel.input.shape[1:3].as_list()\n",
    "        original_img = cv2.imread(thisImg_path) \n",
    "        resized_original_image = cv2.resize(original_img, (width,height))        \n",
    "    \n",
    "        input_image = img_to_array(resized_original_image)\n",
    "        input_image = np.array(input_image, dtype=\"float\") /255.0       \n",
    "        input_image = input_image.reshape((1,) + input_image.shape)\n",
    "    \n",
    "        class_weights = thisModel.layers[-1].get_weights()[0]\n",
    "    \n",
    "        get_output = K.function([thisModel.layers[0].input], [thisModel.layers[-4].output, \n",
    "                                 thisModel.layers[-1].output])\n",
    "        [conv_outputs, predictions] = get_output([input_image])\n",
    "        conv_outputs = conv_outputs[ 0, :, :, :]     \n",
    "        final_output = predictions   \n",
    "        \n",
    "        wf0 = np.zeros(dtype = np.float32, shape = conv_outputs.shape[0:2])    \n",
    "        wf1 = np.zeros(dtype = np.float32, shape = conv_outputs.shape[0:2])    \n",
    "    \n",
    "        for i, w in enumerate(class_weights[:, 0]):     \n",
    "            wf0 += w * conv_outputs[:, :, i]           \n",
    "        S0 = np.sum(wf0)           # score at node 0 in the final output layer\n",
    "        for i, w in enumerate(class_weights[:, 1]):     \n",
    "            wf1 += w * conv_outputs[:, :, i]             \n",
    "        S1 = np.sum(wf1)           # score at node 1 in the final output layer\n",
    "    \n",
    "        #Calculate incremental MSE\n",
    "        iMSE0 = np.zeros(dtype = np.float32, shape = conv_outputs.shape[0:2])\n",
    "        iMSE1 = np.zeros(dtype = np.float32, shape = conv_outputs.shape[0:2])\n",
    "    \n",
    "        row, col = wf0.shape\n",
    "        for x in range (row):\n",
    "                for y in range (col):\n",
    "                        tmp0 = np.array(wf0)\n",
    "                        tmp0[x,y] = 0.                   # remove activation at the spatial location (x,y)\n",
    "                        iMSE0[x,y] = (S0 - np.sum(tmp0)) ** 2\n",
    "    \n",
    "                        tmp1 = np.array(wf1)\n",
    "                        tmp1[x,y] = 0.                  \n",
    "                        iMSE1[x,y] = (S1 - np.sum(tmp1)) ** 2\n",
    "         \n",
    "      \n",
    "        Final_crm = iMSE0 + iMSE1       # consider both positive and negative contribution\n",
    "    \n",
    "        Final_crm /= np.max(Final_crm)    # normalize\n",
    "        resized_Final_crm = cv2.resize(Final_crm, (height, width)) # upscaling to original image size\n",
    "\n",
    "        The_CRM = np.array(resized_Final_crm)\n",
    "        The_CRM[np.where(resized_Final_crm < Threshold)] = 0.  # clean-up (remove data below threshold)\n",
    "\n",
    "        return[resized_original_image, final_output, resized_Final_crm, The_CRM]\n",
    "    except Exception as e:\n",
    "        raise Exception('Error from Generate_CRM_2Class(): ' + str(e))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Generate CRM visualization with a single image\n",
    "\n",
    "InImage1, OutScores1, aCRM_Img1, tCRM_Img1 = Generate_CRM_2Class(vgg16_custom_model,\n",
    "                                                                 img_path, 0.1) \n",
    "plt.figure() \n",
    "plt.imshow(InImage1)\n",
    "aHeatmap = cv2.applyColorMap(np.uint8(255*aCRM_Img1), cv2.COLORMAP_JET)\n",
    "aHeatmap[np.where(aCRM_Img1 < 0.2)] = 0   \n",
    "superimposed_img = aHeatmap * 0.4 + InImage1 \n",
    "\n",
    "#if we have to increse the DPI and write to disk\n",
    "writePNGwithdpi(superimposed_img, \"vgg16_vis.png\", (300,300))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#running a loop to generate activations for a group of images\n",
    "\n",
    "source = glob.glob(\"data\\*.png\")\n",
    "source.sort()\n",
    "\n",
    "#load model\n",
    "vgg16_custom_model = load_model('weights1/vgg16_finetuned.h5')\n",
    "vgg16_custom_model.summary()\n",
    "\n",
    "#compile the model\n",
    "sgd = SGD(lr=1e-3, decay=1e-6, momentum=0.9, nesterov=True) #optimize to your requirements\n",
    "vgg16_custom_model.compile(optimizer=sgd,\n",
    "              loss='categorical_crossentropy',\n",
    "              metrics=['accuracy'])\n",
    "\n",
    "for f in source:\n",
    "    img = image.load_img(f)\n",
    "    img_name = f.split(os.sep)[-1]\n",
    "    \n",
    "    #preprocess the image\n",
    "    x = image.img_to_array(img)\n",
    "    x = np.expand_dims(x, axis=0)\n",
    "    x /= 255 \n",
    "    \n",
    "    #predict on the image\n",
    "    preds = vgg16_custom_model.predict(x)[0]\n",
    "    print(preds)\n",
    "    InImage1, OutScores1, aCRM_Img1, tCRM_Img1 = Generate_CRM_2Class(vgg16_custom_model,\n",
    "                                                                 f, 0.1) \n",
    "    aHeatmap = cv2.applyColorMap(np.uint8(255*aCRM_Img1), cv2.COLORMAP_JET)\n",
    "    aHeatmap[np.where(aCRM_Img1 < 0.2)] = 0   \n",
    "    superimposed_img = aHeatmap * 0.4 + InImage1 \n",
    "    \n",
    "    #if we have to increse the DPI and write to disk\n",
    "    writePNGwithdpi(superimposed_img, \n",
    "                    \"C:/Users/codes/activation/{}_crm_activation.png\".format(img_name[:-4]), (300,300))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Feature embedding visualization using t-SNE: \n",
    "We performed  t-SNE visualization using the best performing model. Here, we embed the 512 (block5-conv3) dimensional feature space (features extracted from the deepest convolutional layer) into 2- dimensions and visualize how the features for the normal and TB classes are clustered in the 2-D feature space. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#%% load the fine-tuned model\n",
    "base_model = load_model ('weights1/vgg16_finetuned.h5') #path to your model\n",
    "base_model.summary()\n",
    "\n",
    "#extract from the GAP layer\n",
    "model = Model(inputs=base_model.input, \n",
    "                    outputs=base_model.get_layer('global_average_pooling2d_1').output)\n",
    "model.summary()\n",
    "\n",
    "#compile the model\n",
    "sgd = SGD(lr=0.001, decay=1e-6, momentum=0.9, nesterov=True) \n",
    "model.compile(optimizer=sgd, loss='categorical_crossentropy')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### We will define the image features. The definition runs the given image_files to model and returns the weights (filters) as a 1, 512 dimension vector from the output of the GAP layer. Since the model was trained as a image of 256x256, every new image is required to go through the same transformation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_image_features(image_file_name):\n",
    "    \n",
    "    image_features = np.zeros((1, 512)) # comes from GAP\n",
    "    im = cv2.resize(cv2.imread(image_file_name), (256,256))\n",
    "    im = im.astype(np.float32, copy=False)\n",
    "    im = np.expand_dims(im, axis=0)  \n",
    "    image_features[0,:] = model.predict(im)[0]\n",
    "    return image_features\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Now lets get the path to the data and compute the image features. In our case, the main folder will be 'test' and the classes will be 'TB' and 'normal'. The features will be extracted from the test data and shown as embeddings. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "PATH=os.getcwd()\n",
    "data_path = os.path.join(PATH, 'test') \n",
    "data_dir_list = os.listdir(data_path)\n",
    "image_features_list=[] #create a list to store image features\n",
    "\n",
    "for dataset in data_dir_list:\n",
    "    img_list=os.listdir(data_path+'/'+ dataset)\n",
    "    print ('Extracting Features of dataset-'+'{}\\n'.format(dataset))\n",
    "    for img in img_list:\n",
    "        image_features=get_image_features(data_path + '/'+ dataset + '/'+ img )\n",
    "        image_features_list.append(image_features)\n",
    "    \n",
    "    \n",
    "image_features_arr=np.asarray(image_features_list)\n",
    "image_features_arr = np.rollaxis(image_features_arr,1,0)\n",
    "image_features_arr = image_features_arr[0,:,:]\n",
    "\n",
    "np.savetxt('feature_vectors_vgg16.txt',image_features_arr)\n",
    "pickle.dump(image_features_arr, open('feature_vectors_vgg16.pkl', 'wb'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Now that we have extracted the features, we will now proceed to visualize the feature embeddings using Tensorboard. The embeddings shows not only the features but also the images corresponding to those respective features that are overlaid on the features. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# path where the embedding logs will be saved\n",
    "PATH = os.getcwd()\n",
    "LOG_DIR = PATH+ '/embedding'\n",
    "data_path = os.path.join(PATH, 'test') #change to test and see\n",
    "data_dir_list = os.listdir(data_path)\n",
    "img_data=[]\n",
    "for dataset in data_dir_list:\n",
    "    img_list=os.listdir(data_path+'/'+ dataset)\n",
    "    print ('Loaded the images of dataset-'+'{}\\n'.format(dataset))\n",
    "    for img in img_list:\n",
    "        input_img=cv2.imread(data_path + '/'+ dataset + '/'+ img )\n",
    "        input_img_resize=cv2.resize(input_img,(256,256)) \n",
    "        #this size can be anything depends on how clearly we want to visualize the images on the tensorbord. \n",
    "        img_data.append(input_img_resize)\n",
    "\n",
    "#convert the images to array  \n",
    "img_data = np.array(img_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#%% #Let us now load the extracted features and store them in a tensorflow variable\n",
    "\n",
    "feature_vectors = np.loadtxt('feature_vectors_vgg16.txt')\n",
    "print (\"feature_vectors_shape:\",feature_vectors.shape)\n",
    "print (\"num of images:\",feature_vectors.shape[0])\n",
    "print (\"size of individual feature vector:\",feature_vectors.shape[1])\n",
    "\n",
    "num_of_samples=feature_vectors.shape[0]\n",
    "num_of_samples_each_class = 10 #TB and Normal test images count\n",
    "\n",
    "features = tf.Variable(feature_vectors, name='features')\n",
    "\n",
    "#let us give lables for the classes and the names for the categories. \n",
    "#See the order in which the images where loaded before.\n",
    "\n",
    "y = np.ones((num_of_samples,),dtype='int64')\n",
    "y[0:10]=0 # TB\n",
    "y[10:]=1 #normal CXRs\n",
    "names = ['TB','Normal']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Let us store the class labels and the numbers now. This gives information on which features belong to which labels and it keeps both the names and the numbers. They assume equal number of images across each classes here. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "metadata_file = open(os.path.join(LOG_DIR, 'metadata_2_classes.tsv'), 'w')\n",
    "metadata_file.write('Class\\tName\\n')\n",
    "k=10 # num of samples in each class\n",
    "j=0\n",
    "for i in range(num_of_samples):\n",
    "    c = names[y[i]]\n",
    "    if i%k==0:\n",
    "        j=j+1\n",
    "    metadata_file.write('{}\\t{}\\n'.format(j,c))\n",
    "metadata_file.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### The following definition file creates the combined image along with any necessary padding. The input arguments are: data: NxHxWX3 tensor containing the images and it returns data: Properly shaped HxWx3 image with any necessary padding.All the images of all the classes will be added to the combined image."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def images_to_combine(data):\n",
    " \n",
    "    if len(data.shape) == 3: \n",
    "        data = np.tile(data[...,np.newaxis], (1,1,1,3))\n",
    "    data = data.astype(np.float32)\n",
    "    min = np.min(data.reshape((data.shape[0], -1)), axis=1)\n",
    "    data = (data.transpose(1,2,3,0) - min).transpose(3,0,1,2)\n",
    "    max = np.max(data.reshape((data.shape[0], -1)), axis=1)\n",
    "    data = (data.transpose(1,2,3,0) / max).transpose(3,0,1,2)\n",
    "    n = int(np.ceil(np.sqrt(data.shape[0])))\n",
    "    padding = ((0, n ** 2 - data.shape[0]), (0, 0),\n",
    "            (0, 0)) + ((0, 0),) * (data.ndim - 3)\n",
    "    data = np.pad(data, padding, mode='constant',\n",
    "            constant_values=0)\n",
    "    # Tile the individual thumbnails into an image.\n",
    "    data = data.reshape((n, n) + data.shape[1:]).transpose((0, 2, 1, 3)\n",
    "            + tuple(range(4, data.ndim + 1)))\n",
    "    data = data.reshape((n * data.shape[1], n * data.shape[3]) + data.shape[4:])\n",
    "    data = (data * 255).astype(np.uint8)\n",
    "    return data\n",
    "\n",
    "combine = images_to_combine(img_data)\n",
    "cv2.imwrite(os.path.join(LOG_DIR, 'combined_TB_normal_classes.png'), combine)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#%% #Now lets create the logs for the features along with the labels to visualize the embeddings.\n",
    "with tf.Session() as sess:\n",
    "    saver = tf.train.Saver([features])\n",
    "\n",
    "    sess.run(features.initializer)\n",
    "    saver.save(sess, os.path.join(LOG_DIR, 'images_2_classes.ckpt'))\n",
    "\n",
    "    config = projector.ProjectorConfig()\n",
    "    # One can add multiple embeddings.\n",
    "    embedding = config.embeddings.add()\n",
    "    embedding.tensor_name = features.name\n",
    "    # Link this tensor to its metadata file (e.g. labels).\n",
    "    embedding.metadata_path = os.path.join(LOG_DIR, 'metadata_2_classes.tsv')\n",
    "    #embedding.metadata_path = metadata\n",
    "    # Saves a config file that TensorBoard will read during startup.\n",
    "    projector.visualize_embeddings(tf.summary.FileWriter(LOG_DIR), config)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### To run the embeddings launch tensor board (tensorboard --logdir=embedding --port=6006). Please make sure there is no gap between the name of your directory- for e.g.- folder name will not work it has to be folder_name. Then open localhost:6006 in a browser and go to the embedding options in Tensorboard. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Bone Suppression:\n",
    "\n",
    "The researchers from the Budapest University of Technology and Economics used their in-house clavicle and rib-shadow removal algorithms to suppress the bones in the 247 JSRT CXRs and made the bone-suppressed soft-tissue images publicly available. \n",
    "\n",
    "We used affine transformations including rotations, horizontal and vertical shifting, horizontal mirroring, zooming, median, maximum, and minimum, and unsharp masking to generate 4500 image pairs offline from this initial set of CXRs and their bone-suppressed counterparts. The augmented images are resized to 256 Ã— 256 spatial resolution. We enhanced image contrast by saturating the bottom and top 1% of all image pixel values. The grayscale pixel values are then normalized. \n",
    "\n",
    "We trained several customized ConvNet-based bone suppression models with varying architecture on this augmented dataset and evaluated their performance with the cross-institutional NIH-CC-DES test set. During training, we allocated 10% of the training data for validation using a fixed seed. \n",
    "\n",
    "We propose four different model architecture toward the task of bone suppression in CXRs as follows: (a) Autoencoder (AE) model (AE-BS) where BS denotes bone suppression; (b) Sequential ConvNet model (ConvNet-BS); (c) Residual learning model (RL-BS); and (d) Residual network model (ResNet-BS). \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "#%% loss functions\n",
    "\n",
    "def tf_log10(x):\n",
    "    numerator = tf.log(x)\n",
    "    denominator = tf.log(tf.constant(10, dtype=numerator.dtype))\n",
    "    return numerator / denominator\n",
    "\n",
    "def PSNR(y_true, y_pred):\n",
    "    max_pixel = 1.0\n",
    "    return 10.0 * tf_log10((max_pixel ** 2) / (K.mean(K.square(y_pred - y_true)))) \n",
    "\n",
    "def mae(y_true, y_pred):\n",
    "    return mean_absolute_error(y_true, y_pred)\n",
    "\n",
    "def ssim(y_true, y_pred):\n",
    "    return tf.reduce_mean(tf.image.ssim(y_true, y_pred, 1.0))\n",
    "\n",
    "def ssim_loss(y_true, y_pred):\n",
    "    return 1-tf.reduce_mean(tf.image.ssim(y_true, y_pred, 1.0))\n",
    "\n",
    "def ssim_multi(y_true, y_pred):\n",
    "    return tf.reduce_mean(tf.image.ssim_multiscale(y_true, y_pred, 1.0))\n",
    "\n",
    "def ssim_multi_loss(y_true, y_pred):\n",
    "    return 1-tf.reduce_mean(tf.image.ssim_multiscale(y_true, y_pred, 1.0))\n",
    "        \n",
    "def loss_mix_multi_084(y_true, y_pred):    \n",
    "    return 0.16 * mean_absolute_error(y_true, y_pred) + \\\n",
    "            0.84 * (1-ssim_multi(y_true, y_pred)) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#%% load the data and split into training and validation images\n",
    "\n",
    "def load_data(no_of_images):\n",
    "    \n",
    "    img_size = (256, 256)\n",
    "    imgs_source = [] \n",
    "    imgs_target = []\n",
    "    \n",
    "    dir_source = \"data/augmented/train\"\n",
    "    dir_target = \"data/augmented/val\"\n",
    "    \n",
    "    i = 0\n",
    "    for _, _, filenames in os.walk('data/augmented/train/'):\n",
    "        for filename in filenames:\n",
    "            i = i+1\n",
    "            if(i > no_of_images):\n",
    "                break\n",
    "            img_source = cv2.imread(os.path.join(dir_source,filename),cv2.IMREAD_GRAYSCALE)\n",
    "            img_target = cv2.imread(os.path.join(dir_target, filename),cv2.IMREAD_GRAYSCALE)\n",
    "            \n",
    "            # resizing images\n",
    "            img_source = cv2.resize(img_source,img_size)\n",
    "            img_target = cv2.resize(img_target,img_size)\n",
    "            \n",
    "            # normalizing images\n",
    "            img_source = np.array(img_source)/255\n",
    "            img_target = np.array(img_target)/255\n",
    "            \n",
    "            imgs_source.append(img_source)\n",
    "            imgs_target.append(img_target)\n",
    "            \n",
    "    return imgs_source, imgs_target\n",
    "\n",
    "source, target = load_data(4500) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#%%Reshaping and Spliting the Data when using a common folder; perform train and validation split with a random seed\n",
    "\n",
    "img_rows = 256\n",
    "img_cols = 256\n",
    "img_channels = 1\n",
    "img_shape = (img_rows, img_cols, img_channels)\n",
    "\n",
    "source = np.array(source).reshape(-1, img_rows, img_cols, img_channels)\n",
    "target = np.array(target).reshape(-1, img_rows, img_cols, img_channels)\n",
    "\n",
    "source_train, source_val, target_train, target_val = train_test_split(source, target,\n",
    "                                                                        test_size=0.1,\n",
    "                                                                        random_state=13) #10% for validation with a fixed seed\n",
    "print(source_train.shape, source_val.shape, target_train.shape, target_val.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#%% load test data\n",
    "\n",
    "def load_test_data(no_of_images):\n",
    "    \n",
    "    img_size = (256, 256)\n",
    "    imgs_source = []\n",
    "    imgs_target = []\n",
    "    \n",
    "    dir_source = \"data/augmented/test_source\"\n",
    "    dir_target = \"data/augmented/test_target\"\n",
    "    \n",
    "    \n",
    "    i = 0\n",
    "    for _, _, filenames in os.walk('data/augmented/test_source/'):\n",
    "        for filename in filenames:\n",
    "            i = i+1\n",
    "            if(i > no_of_images):\n",
    "                break\n",
    "            img_source = cv2.imread(os.path.join(dir_source,filename),cv2.IMREAD_GRAYSCALE)\n",
    "            img_target = cv2.imread(os.path.join(dir_target, filename),cv2.IMREAD_GRAYSCALE)\n",
    "            \n",
    "            # resizing images\n",
    "            img_source = cv2.resize(img_source,img_size)\n",
    "            img_target = cv2.resize(img_target,img_size)\n",
    "            \n",
    "            # normalizing images\n",
    "            img_source = np.array(img_source)/255\n",
    "            img_target = np.array(img_target)/255\n",
    "            \n",
    "            imgs_source.append(img_source)\n",
    "            imgs_target.append(img_target)\n",
    "    \n",
    "    return imgs_source, imgs_target\n",
    "\n",
    "source_test, target_test = load_test_data(27) #number of test images\n",
    "print(source_test)\n",
    "print(target_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#print the number and shape of the data: train and test\n",
    "\n",
    "img_rows = 256\n",
    "img_cols = 256\n",
    "img_channels = 1\n",
    "img_shape = (img_rows, img_cols, img_channels)\n",
    "\n",
    "source_test = np.array(source_test).reshape(-1, img_rows, img_cols, img_channels)\n",
    "target_test = np.array(target_test).reshape(-1, img_rows, img_cols, img_channels)\n",
    "print(source_test.shape, target_test.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Train each of the proposed bone-suppression models on the input data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# building the models\n",
    "# AE-BS model:\n",
    "\n",
    "def autoencoder_base(input_img):\n",
    "    #encoder\n",
    "    conv1 = Conv2D(16, (5, 5), activation='relu', padding='same')(input_img) \n",
    "    conv2 = Conv2D(32, (5, 5), activation='relu', padding='same', strides=2)(conv1)\n",
    "    conv3 = Conv2D(64, (5, 5), activation='relu', padding='same', strides=2)(conv2)\n",
    "    #decoder\n",
    "    conv4 = Conv2D(32, (5, 5), activation='relu', padding='same')(conv3) \n",
    "    up1 = UpSampling2D((2,2))(conv4) \n",
    "    conv5 = Conv2D(16, (5, 5), activation='relu', padding='same')(up1)\n",
    "    up2 = UpSampling2D((2,2))(conv5) \n",
    "    decoded = Conv2D(1, (5, 5), activation='sigmoid', padding='same')(up2) \n",
    "    return decoded\n",
    "\n",
    "input_img = Input(shape = img_shape)\n",
    "ae_base = Model(input_img, autoencoder_base(input_img), name='AE-BS')\n",
    "ae_base.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#%% train the AE-BS model\n",
    "n_epoch = 200\n",
    "n_batch = 8\n",
    "\n",
    "filepath='weights/' + ae_base.name +'.bestjsrt4500.h5' \n",
    "checkpoint = ModelCheckpoint(filepath, monitor='val_loss', \n",
    "                             verbose=1, save_weights_only=True,\n",
    "                             save_best_only=True, mode='min') \n",
    "earlyStopping = EarlyStopping(monitor='val_loss', \n",
    "                               patience=10, verbose=1, mode='min')\n",
    "tensor_board = TensorBoard(log_dir='logs/', histogram_freq=0, batch_size=n_batch)\n",
    "reduce_lr = ReduceLROnPlateau(monitor='val_loss', factor=0.5, patience=10,\n",
    "                              verbose=1, mode='min', min_lr=0.00001)\n",
    "callbacks_list = [checkpoint, tensor_board, earlyStopping, reduce_lr]\n",
    "\n",
    "#BEGIN TRAINING\n",
    "t=time.time()\n",
    "ae_base.compile(optimizer=Adam(lr=0.001), loss=loss_mix_multi_084, \n",
    "              metrics=[mae, ssim_multi_loss,\n",
    "                       PSNR, ssim, ssim_multi]) \n",
    "\n",
    "ae_base_train = ae_base.fit(source_train, target_train,\n",
    "                                    epochs = n_epoch,\n",
    "                                    batch_size = n_batch,\n",
    "                                    verbose = 1,\n",
    "                                    callbacks=callbacks_list,\n",
    "                                    shuffle=True,\n",
    "                                    validation_data = (source_val, target_val))\n",
    "\n",
    "print('Training time: %s' % (time.time()-t))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ae_base.metric_names"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#plot performance\n",
    "N = 200 #modify if early stopping\n",
    "plt.style.use(\"ggplot\")\n",
    "plt.figure(figsize=(20,10), dpi=100)\n",
    "plt.plot(np.arange(1, N+1), \n",
    "         ae_base_train.history[\"loss\"], 'orange', label=\"train_combined_loss\")\n",
    "plt.plot(np.arange(1, N+1), \n",
    "         ae_base_train.history[\"val_loss\"], 'red', label=\"val_combined_loss\")\n",
    "plt.plot(np.arange(1, N+1), \n",
    "         ae_base_train.history[\"mae\"], 'blue', label=\"MAE_loss\")\n",
    "plt.plot(np.arange(1, N+1), \n",
    "         ae_base_train.history[\"ssim_multi_loss\"], 'green', label=\"MS-SSIM_loss\")\n",
    "plt.xlabel(\"Epoch #\")\n",
    "plt.ylabel(\"Loss\")\n",
    "plt.legend(loc=\"lower right\")\n",
    "plt.savefig(\"AE-BS_performance.png\", dpi=600)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#evaluation\n",
    "#load the best model:\n",
    "ae_base.load_weights('weights/AE-BS.bestjsrt4500.h5')\n",
    "ae_base.summary()\n",
    "\n",
    "#evaluate metrics with the validation data\n",
    "score_ae_val = ae_base.evaluate(source_val, target_val, verbose = 1) \n",
    "print('The value of combined loss is:', score_ae_val[0]) \n",
    "print('The Mean absolute error value is:', score_ae_val[1])\n",
    "print('The value of MS-SSIM Loss is:', score_ae_val[2])\n",
    "print('The PSNR value is:', score_ae_val[3])\n",
    "print('The SSIM value is:', score_ae_val[4])\n",
    "print('The value of MS-SSIM metric is:', score_ae_val[5])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#evaluate with the test data\n",
    "score_ae_test = ae_base.evaluate(source_test, target_test, verbose = 1) \n",
    "print('The value of combined loss is:', score_ae_test[0]) \n",
    "print('The Mean absolute error value is:', score_ae_test[1])\n",
    "print('The value of MS-SSIM Loss is:', score_ae_test[2])\n",
    "print('The PSNR value is:', score_ae_test[3])\n",
    "print('The SSIM value is:', score_ae_test[4])\n",
    "print('The value of MS-SSIM metric is:', score_ae_test[5])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# CNN-BS model:\n",
    "from keras import regularizers\n",
    "def cnn_base(input_img):\n",
    "    conv1 = Conv2D(16, (3, 3), activation='relu', padding='same',\n",
    "                   kernel_regularizer=regularizers.l1(10e-10))(input_img) \n",
    "    conv2 = Conv2D(32, (3, 3), activation='relu', padding='same',\n",
    "                   kernel_regularizer=regularizers.l1(10e-10))(conv1) \n",
    "    conv3 = Conv2D(64, (3, 3), activation='relu', padding='same',\n",
    "                   kernel_regularizer=regularizers.l1(10e-10))(conv2)\n",
    "    conv4 = Conv2D(126, (3, 3), activation='relu', padding='same',\n",
    "                   kernel_regularizer=regularizers.l1(10e-10))(conv3)\n",
    "    conv5 = Conv2D(256, (3, 3), activation='relu', padding='same',\n",
    "                   kernel_regularizer=regularizers.l1(10e-10))(conv4) \n",
    "    conv6 = Conv2D(512, (3, 3), activation='relu', padding='same',\n",
    "                   kernel_regularizer=regularizers.l1(10e-10))(conv5)\n",
    "    conv7 = Conv2D(1, (3, 3), activation='sigmoid', padding='same',\n",
    "                   kernel_regularizer=regularizers.l1(10e-10))(conv6) \n",
    "    return conv7\n",
    "\n",
    "input_img = Input(shape = img_shape)\n",
    "cnn_base = Model(input_img, cnn_base(input_img), name='CNN-BS')\n",
    "cnn_base.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#%% train the CNN-BS model\n",
    "n_epoch = 200\n",
    "n_batch = 8\n",
    "\n",
    "filepath='weights1/' + cnn_base.name +'.bestjsrt4500.h5' \n",
    "checkpoint = ModelCheckpoint(filepath, monitor='val_loss', \n",
    "                             verbose=1, save_weights_only=True,\n",
    "                             save_best_only=True, mode='min') \n",
    "earlyStopping = EarlyStopping(monitor='val_loss', \n",
    "                               patience=10, verbose=1, mode='min')\n",
    "tensor_board = TensorBoard(log_dir='logs/', histogram_freq=0, batch_size=n_batch)\n",
    "reduce_lr = ReduceLROnPlateau(monitor='val_loss', factor=0.5, patience=10,\n",
    "                              verbose=1, mode='min', min_lr=0.00001)\n",
    "callbacks_list = [checkpoint, tensor_board, earlyStopping, reduce_lr]\n",
    "\n",
    "#BEGIN TRAINING\n",
    "t=time.time()\n",
    "cnn_base.compile(optimizer=Adam(lr=0.001), loss=loss_mix_multi_084, \n",
    "              metrics=[mae, ssim_multi_loss,\n",
    "                       PSNR, ssim, ssim_multi]) \n",
    "\n",
    "cnn_base_train = cnn_base.fit(source_train, target_train,\n",
    "                                    epochs = n_epoch,\n",
    "                                    batch_size = n_batch,\n",
    "                                    verbose = 1,\n",
    "                                    callbacks=callbacks_list,\n",
    "                                    shuffle=True,\n",
    "                                    validation_data = (source_val, target_val))\n",
    "\n",
    "print('Training time: %s' % (time.time()-t))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cnn_base.metric_names"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#plot performance\n",
    "N = 200 #modify if early stopping\n",
    "plt.style.use(\"ggplot\")\n",
    "plt.figure(figsize=(20,10), dpi=100)\n",
    "plt.plot(np.arange(1, N+1), \n",
    "         cnn_base_train.history[\"loss\"], 'orange', label=\"train_combined_loss\")\n",
    "plt.plot(np.arange(1, N+1), \n",
    "         cnn_base_train.history[\"val_loss\"], 'red', label=\"val_combined_loss\")\n",
    "plt.plot(np.arange(1, N+1), \n",
    "         cnn_base_train.history[\"mae\"], 'blue', label=\"MAE_loss\")\n",
    "plt.plot(np.arange(1, N+1), \n",
    "         cnn_base_train.history[\"ssim_multi_loss\"], 'green', label=\"MS-SSIM_loss\")\n",
    "plt.xlabel(\"Epoch #\")\n",
    "plt.ylabel(\"Loss\")\n",
    "plt.legend(loc=\"lower right\")\n",
    "plt.savefig(\"CNN-BS_performance.png\", dpi=600)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#evaluation\n",
    "#load the best model:\n",
    "cnn_base.load_weights('weights/CNN-BS.bestjsrt4500.h5')\n",
    "cnn_base.summary()\n",
    "\n",
    "#evaluate metrics with the validation data\n",
    "score_cnn_val = cnn_base.evaluate(source_val, target_val, verbose = 1) \n",
    "print('The value of combined loss is:', score_cnn_val[0]) \n",
    "print('The Mean absolute error value is:', score_cnn_val[1])\n",
    "print('The value of MS-SSIM Loss is:', score_cnn_val[2])\n",
    "print('The PSNR value is:', score_cnn_val[3])\n",
    "print('The SSIM value is:', score_cnn_val[4])\n",
    "print('The value of MS-SSIM metric is:', score_cnn_val[5])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#evaluate metrics with the test data\n",
    "score_cnn_test = cnn_base.evaluate(source_val, target_val, verbose = 1) \n",
    "print('The value of combined loss is:', score_cnn_test[0]) \n",
    "print('The Mean absolute error value is:', score_cnn_test[1])\n",
    "print('The value of MS-SSIM Loss is:', score_cnn_test[2])\n",
    "print('The PSNR value is:', score_cnn_test[3])\n",
    "print('The SSIM value is:', score_cnn_test[4])\n",
    "print('The value of MS-SSIM metric is:', score_cnn_test[5])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#RL-BS model:\n",
    "def rl_base(input_size=(256,256,1)):     \n",
    "    model_input = Input(shape=(256,256,1))  \n",
    "    model = Conv2D(8, (3, 3), padding='same', \n",
    "                   activation='relu')(model_input)\n",
    "    model = Conv2D(16, (3, 3), padding='same', \n",
    "                   activation='relu')(model)\n",
    "    model = Conv2D(32, (3, 3), padding='same', \n",
    "                   activation='relu')(model)\n",
    "    model = Conv2D(64, (3, 3), padding='same', \n",
    "                   activation='relu', \n",
    "                   dilation_rate=2)(model)\n",
    "    model = Conv2D(128, (3, 3), padding='same', \n",
    "                   activation='relu')(model)\n",
    "    model = Conv2D(256, (3, 3), padding='same', \n",
    "                   activation='relu')(model)\n",
    "    model = Conv2D(512, (3, 3), padding='same', \n",
    "                   activation='relu')(model)\n",
    "    model = Conv2D(1, (3, 3), padding='same')(model)  \n",
    "    res_img = model\n",
    "    output_img = add([res_img, model_input])\n",
    "    model = Model(inputs = model_input, outputs = output_img, name='RL-BS')\n",
    "    return model\n",
    "\n",
    "RL_BS = rl_base(input_size=(256,256,1))\n",
    "print(RL_BS.summary())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#%% train the RL-BS model\n",
    "n_epoch = 200\n",
    "n_batch = 8\n",
    "\n",
    "filepath='weights1/' + RL_BS.name +'.bestjsrt4500.h5' \n",
    "checkpoint = ModelCheckpoint(filepath, monitor='val_loss', \n",
    "                             verbose=1, save_weights_only=True,\n",
    "                             save_best_only=True, mode='min') \n",
    "earlyStopping = EarlyStopping(monitor='val_loss', \n",
    "                               patience=10, verbose=1, mode='min')\n",
    "tensor_board = TensorBoard(log_dir='logs/', histogram_freq=0, batch_size=n_batch)\n",
    "reduce_lr = ReduceLROnPlateau(monitor='val_loss', factor=0.5, patience=10,\n",
    "                              verbose=1, mode='min', min_lr=0.00001)\n",
    "callbacks_list = [checkpoint, tensor_board, earlyStopping, reduce_lr]\n",
    "\n",
    "#BEGIN TRAINING\n",
    "t=time.time()\n",
    "RL_BS.compile(optimizer=Adam(lr=0.001), loss=loss_mix_multi_084, \n",
    "              metrics=[mae, ssim_multi_loss,\n",
    "                       PSNR, ssim, ssim_multi]) \n",
    "\n",
    "RL_BS_train = RL_BS.fit(source_train, target_train,\n",
    "                                    epochs = n_epoch,\n",
    "                                    batch_size = n_batch,\n",
    "                                    verbose = 1,\n",
    "                                    callbacks=callbacks_list,\n",
    "                                    shuffle=True,\n",
    "                                    validation_data = (source_val, target_val))\n",
    "\n",
    "print('Training time: %s' % (time.time()-t))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "RL_BS.metric_names"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#plot performance\n",
    "N = 200 #modify if early stopping\n",
    "plt.style.use(\"ggplot\")\n",
    "plt.figure(figsize=(20,10), dpi=100)\n",
    "plt.plot(np.arange(1, N+1), \n",
    "         RL_BS_train.history[\"loss\"], 'orange', label=\"train_combined_loss\")\n",
    "plt.plot(np.arange(1, N+1), \n",
    "         RL_BS_train.history[\"val_loss\"], 'red', label=\"val_combined_loss\")\n",
    "plt.plot(np.arange(1, N+1), \n",
    "         RL_BS_train.history[\"mae\"], 'blue', label=\"MAE_loss\")\n",
    "plt.plot(np.arange(1, N+1), \n",
    "         RL_BS_train.history[\"ssim_multi_loss\"], 'green', label=\"MS-SSIM_loss\")\n",
    "plt.xlabel(\"Epoch #\")\n",
    "plt.ylabel(\"Loss\")\n",
    "plt.legend(loc=\"lower right\")\n",
    "plt.savefig(\"RL-BS_performance.png\", dpi=600)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#evaluation\n",
    "#load the best model:\n",
    "RL_BS.load_weights('weights/RL-BS.bestjsrt4500.h5')\n",
    "RL_BS.summary()\n",
    "\n",
    "#evaluate metrics with the validation data\n",
    "score_RL_BS_val = RL_BS.evaluate(source_val, target_val, verbose = 1) \n",
    "print('The value of combined loss is:', score_RL_BS_val[0]) \n",
    "print('The Mean absolute error value is:', score_RL_BS_val[1])\n",
    "print('The value of MS-SSIM Loss is:', score_RL_BS_val[2])\n",
    "print('The PSNR value is:', score_RL_BS_val[3])\n",
    "print('The SSIM value is:', score_RL_BS_val[4])\n",
    "print('The value of MS-SSIM metric is:', score_RL_BS_val[5])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#evaluate metrics with the validation data\n",
    "score_RL_BS_test = RL_BS.evaluate(source_test, target_test, verbose = 1) \n",
    "print('The value of combined loss is:', score_RL_BS_test[0]) \n",
    "print('The Mean absolute error value is:', score_RL_BS_test[1])\n",
    "print('The value of MS-SSIM Loss is:', score_RL_BS_test[2])\n",
    "print('The PSNR value is:', score_RL_BS_test[3])\n",
    "print('The SSIM value is:', score_RL_BS_test[4])\n",
    "print('The value of MS-SSIM metric is:', score_RL_BS_test[5])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "__________________________________________________________________________________________________\n",
      "Layer (type)                    Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      "input_2 (InputLayer)            (None, 256, 256, 1)  0                                            \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_1 (Conv2D)               (None, 256, 256, 64) 640         input_2[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_2 (Conv2D)               (None, 256, 256, 64) 36928       conv2d_1[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_3 (Conv2D)               (None, 256, 256, 64) 36928       conv2d_2[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "lambda_1 (Lambda)               (None, 256, 256, 64) 0           conv2d_3[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "add_1 (Add)                     (None, 256, 256, 64) 0           conv2d_1[0][0]                   \n",
      "                                                                 lambda_1[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_4 (Conv2D)               (None, 256, 256, 64) 36928       add_1[0][0]                      \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_5 (Conv2D)               (None, 256, 256, 64) 36928       conv2d_4[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "lambda_2 (Lambda)               (None, 256, 256, 64) 0           conv2d_5[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "add_2 (Add)                     (None, 256, 256, 64) 0           add_1[0][0]                      \n",
      "                                                                 lambda_2[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_6 (Conv2D)               (None, 256, 256, 64) 36928       add_2[0][0]                      \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_7 (Conv2D)               (None, 256, 256, 64) 36928       conv2d_6[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "lambda_3 (Lambda)               (None, 256, 256, 64) 0           conv2d_7[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "add_3 (Add)                     (None, 256, 256, 64) 0           add_2[0][0]                      \n",
      "                                                                 lambda_3[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_8 (Conv2D)               (None, 256, 256, 64) 36928       add_3[0][0]                      \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_9 (Conv2D)               (None, 256, 256, 64) 36928       conv2d_8[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "lambda_4 (Lambda)               (None, 256, 256, 64) 0           conv2d_9[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "add_4 (Add)                     (None, 256, 256, 64) 0           add_3[0][0]                      \n",
      "                                                                 lambda_4[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_10 (Conv2D)              (None, 256, 256, 64) 36928       add_4[0][0]                      \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_11 (Conv2D)              (None, 256, 256, 64) 36928       conv2d_10[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "lambda_5 (Lambda)               (None, 256, 256, 64) 0           conv2d_11[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "add_5 (Add)                     (None, 256, 256, 64) 0           add_4[0][0]                      \n",
      "                                                                 lambda_5[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_12 (Conv2D)              (None, 256, 256, 64) 36928       add_5[0][0]                      \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_13 (Conv2D)              (None, 256, 256, 64) 36928       conv2d_12[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "lambda_6 (Lambda)               (None, 256, 256, 64) 0           conv2d_13[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "add_6 (Add)                     (None, 256, 256, 64) 0           add_5[0][0]                      \n",
      "                                                                 lambda_6[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_14 (Conv2D)              (None, 256, 256, 64) 36928       add_6[0][0]                      \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_15 (Conv2D)              (None, 256, 256, 64) 36928       conv2d_14[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "lambda_7 (Lambda)               (None, 256, 256, 64) 0           conv2d_15[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "add_7 (Add)                     (None, 256, 256, 64) 0           add_6[0][0]                      \n",
      "                                                                 lambda_7[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_16 (Conv2D)              (None, 256, 256, 64) 36928       add_7[0][0]                      \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_17 (Conv2D)              (None, 256, 256, 64) 36928       conv2d_16[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "lambda_8 (Lambda)               (None, 256, 256, 64) 0           conv2d_17[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "add_8 (Add)                     (None, 256, 256, 64) 0           add_7[0][0]                      \n",
      "                                                                 lambda_8[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_18 (Conv2D)              (None, 256, 256, 64) 36928       add_8[0][0]                      \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_19 (Conv2D)              (None, 256, 256, 64) 36928       conv2d_18[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "lambda_9 (Lambda)               (None, 256, 256, 64) 0           conv2d_19[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "add_9 (Add)                     (None, 256, 256, 64) 0           add_8[0][0]                      \n",
      "                                                                 lambda_9[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_20 (Conv2D)              (None, 256, 256, 64) 36928       add_9[0][0]                      \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_21 (Conv2D)              (None, 256, 256, 64) 36928       conv2d_20[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "lambda_10 (Lambda)              (None, 256, 256, 64) 0           conv2d_21[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "add_10 (Add)                    (None, 256, 256, 64) 0           add_9[0][0]                      \n",
      "                                                                 lambda_10[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_22 (Conv2D)              (None, 256, 256, 64) 36928       add_10[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_23 (Conv2D)              (None, 256, 256, 64) 36928       conv2d_22[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "lambda_11 (Lambda)              (None, 256, 256, 64) 0           conv2d_23[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "add_11 (Add)                    (None, 256, 256, 64) 0           add_10[0][0]                     \n",
      "                                                                 lambda_11[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_24 (Conv2D)              (None, 256, 256, 64) 36928       add_11[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_25 (Conv2D)              (None, 256, 256, 64) 36928       conv2d_24[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "lambda_12 (Lambda)              (None, 256, 256, 64) 0           conv2d_25[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "add_12 (Add)                    (None, 256, 256, 64) 0           add_11[0][0]                     \n",
      "                                                                 lambda_12[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_26 (Conv2D)              (None, 256, 256, 64) 36928       add_12[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_27 (Conv2D)              (None, 256, 256, 64) 36928       conv2d_26[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "lambda_13 (Lambda)              (None, 256, 256, 64) 0           conv2d_27[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "add_13 (Add)                    (None, 256, 256, 64) 0           add_12[0][0]                     \n",
      "                                                                 lambda_13[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_28 (Conv2D)              (None, 256, 256, 64) 36928       add_13[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_29 (Conv2D)              (None, 256, 256, 64) 36928       conv2d_28[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "lambda_14 (Lambda)              (None, 256, 256, 64) 0           conv2d_29[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "add_14 (Add)                    (None, 256, 256, 64) 0           add_13[0][0]                     \n",
      "                                                                 lambda_14[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_30 (Conv2D)              (None, 256, 256, 64) 36928       add_14[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_31 (Conv2D)              (None, 256, 256, 64) 36928       conv2d_30[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "lambda_15 (Lambda)              (None, 256, 256, 64) 0           conv2d_31[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "add_15 (Add)                    (None, 256, 256, 64) 0           add_14[0][0]                     \n",
      "                                                                 lambda_15[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_32 (Conv2D)              (None, 256, 256, 64) 36928       add_15[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_33 (Conv2D)              (None, 256, 256, 64) 36928       conv2d_32[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "lambda_16 (Lambda)              (None, 256, 256, 64) 0           conv2d_33[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "add_16 (Add)                    (None, 256, 256, 64) 0           add_15[0][0]                     \n",
      "                                                                 lambda_16[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_34 (Conv2D)              (None, 256, 256, 64) 36928       add_16[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "add_17 (Add)                    (None, 256, 256, 64) 0           conv2d_1[0][0]                   \n",
      "                                                                 conv2d_34[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_35 (Conv2D)              (None, 256, 256, 1)  577         add_17[0][0]                     \n",
      "==================================================================================================\n",
      "Total params: 1,219,841\n",
      "Trainable params: 1,219,841\n",
      "Non-trainable params: 0\n",
      "__________________________________________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "#ResNet-BS model:\n",
    "\n",
    "def resnet_bs(num_filters=64, num_res_blocks=16, res_block_scaling=None):\n",
    "    x_in = Input(shape=(256,256,1))\n",
    "    x = b = Conv2D(num_filters, (3, 3), padding='same')(x_in)\n",
    "    for i in range(num_res_blocks):\n",
    "        b = res_block(b, num_filters, res_block_scaling)\n",
    "    b = Conv2D(num_filters, (3, 3), padding='same')(b)\n",
    "    x = Add()([x, b])\n",
    "    x = Conv2D(1, (3, 3), padding='same')(x)\n",
    "    return Model(x_in, x, name=\"ResNet-BS\")\n",
    "\n",
    "def res_block(x_in, filters, scaling):\n",
    "    x = Conv2D(filters, (3, 3), padding='same', activation='relu')(x_in)\n",
    "    x = Conv2D(filters, (3, 3), padding='same')(x)\n",
    "    if scaling:\n",
    "        x = Lambda(lambda t: t * scaling)(x)\n",
    "    x = Add()([x_in, x])\n",
    "    return x\n",
    "\n",
    "resnet_bs = resnet_bs(num_filters=64, num_res_blocks=16, res_block_scaling=0.1)\n",
    "resnet_bs.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#%% train the ResNet-BS model\n",
    "n_epoch = 200\n",
    "n_batch = 8\n",
    "\n",
    "filepath='weights1/' + resnet_bs.name +'.bestjsrt4500.h5' \n",
    "checkpoint = ModelCheckpoint(filepath, monitor='val_loss', \n",
    "                             verbose=1, save_weights_only=True,\n",
    "                             save_best_only=True, mode='min') \n",
    "earlyStopping = EarlyStopping(monitor='val_loss', \n",
    "                               patience=10, verbose=1, mode='min')\n",
    "tensor_board = TensorBoard(log_dir='logs/', histogram_freq=0, batch_size=n_batch)\n",
    "reduce_lr = ReduceLROnPlateau(monitor='val_loss', factor=0.5, patience=10,\n",
    "                              verbose=1, mode='min', min_lr=0.00001)\n",
    "callbacks_list = [checkpoint, tensor_board, earlyStopping, reduce_lr]\n",
    "\n",
    "#BEGIN TRAINING\n",
    "t=time.time()\n",
    "resnet_bs.compile(optimizer=Adam(lr=0.001), loss=loss_mix_multi_084, \n",
    "              metrics=[mae, ssim_multi_loss,\n",
    "                       PSNR, ssim, ssim_multi]) \n",
    "\n",
    "resnet_bs_train = resnet_bs.fit(source_train, target_train,\n",
    "                                    epochs = n_epoch,\n",
    "                                    batch_size = n_batch,\n",
    "                                    verbose = 1,\n",
    "                                    callbacks=callbacks_list,\n",
    "                                    shuffle=True,\n",
    "                                    validation_data = (source_val, target_val))\n",
    "\n",
    "print('Training time: %s' % (time.time()-t))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "resnet_bs.metric_names"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#plot performance\n",
    "N = 200 #modify if early stopping\n",
    "plt.style.use(\"ggplot\")\n",
    "plt.figure(figsize=(20,10), dpi=100)\n",
    "plt.plot(np.arange(1, N+1), \n",
    "         resnet_bs_train.history[\"loss\"], 'orange', label=\"train_combined_loss\")\n",
    "plt.plot(np.arange(1, N+1), \n",
    "         resnet_bs_train.history[\"val_loss\"], 'red', label=\"val_combined_loss\")\n",
    "plt.plot(np.arange(1, N+1), \n",
    "         resnet_bs_train.history[\"mae\"], 'blue', label=\"MAE_loss\")\n",
    "plt.plot(np.arange(1, N+1), \n",
    "         resnet_bs_train.history[\"ssim_multi_loss\"], 'green', label=\"MS-SSIM_loss\")\n",
    "plt.xlabel(\"Epoch #\")\n",
    "plt.ylabel(\"Loss\")\n",
    "plt.legend(loc=\"lower right\")\n",
    "plt.savefig(\"ResNet-BS_performance.png\", dpi=600)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#evaluation\n",
    "#load the best model:\n",
    "resnet_bs.load_weights('weights/ResNet-BS.bestjsrt4500.h5')\n",
    "resnet_bs.summary()\n",
    "\n",
    "#evaluate metrics with the validation data\n",
    "score_resnet_bs_val = resnet_bs.evaluate(source_val, target_val, verbose = 1) \n",
    "print('The value of combined loss is:', score_resnet_bs_val[0]) \n",
    "print('The Mean absolute error value is:', score_resnet_bs_val[1])\n",
    "print('The value of MS-SSIM Loss is:', score_resnet_bs_val[2])\n",
    "print('The PSNR value is:', score_resnet_bs_val[3])\n",
    "print('The SSIM value is:', score_resnet_bs_val[4])\n",
    "print('The value of MS-SSIM metric is:', score_resnet_bs_val[5])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#evaluate metrics with the test data\n",
    "score_resnet_bs_test = resnet_bs.evaluate(source_test, target_test, verbose = 1) \n",
    "print('The value of combined loss is:', score_resnet_bs_test[0]) \n",
    "print('The Mean absolute error value is:', score_resnet_bs_test[1])\n",
    "print('The value of MS-SSIM Loss is:', score_resnet_bs_test[2])\n",
    "print('The PSNR value is:', score_resnet_bs_test[3])\n",
    "print('The SSIM value is:', score_resnet_bs_test[4])\n",
    "print('The value of MS-SSIM metric is:', score_resnet_bs_test[5])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Now that the individual bone-suppression models are evalauted and tested here we show how to use the model to predict with a single image using the ResNet-BS model. The same process can be adopted for all models."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#%% predict on a single image\n",
    "#load model:\n",
    "resnet_bs.load_weights(\"weights/ResNet-BS.bestjsrt4500.h5\") \n",
    "print(\"Loaded model from disk\")\n",
    "resnet_bs.summary()\n",
    "\n",
    "#open image\n",
    "img = Image.open('image012.png')\n",
    "img = img.resize((256,256)) \n",
    "x = image.img_to_array(img)\n",
    "x = x.astype('float32') / 255\n",
    "imageio.imwrite('original_image.png',x)\n",
    "x1 = np.expand_dims(x, axis=0)\n",
    "t=time.time()\n",
    "pred = resnet_bs.predict(x1)\n",
    "print('prediction time: %s' % (time.time()-t))\n",
    "test_img = np.reshape(pred, (256,256,1)) #reshape to the original size\n",
    "imageio.imwrite('resnet_bs_pred.png', test_img)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### We can run the bone suppression on a set of images using the best bone-suppression model as shown below"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#%% perform bone suppression in all images\n",
    "source = glob.glob(r'C:\\Users\\test\\*.png')\n",
    "source.sort()\n",
    "\n",
    "#load model\n",
    "resnet_bs.load_weights(\"weights/ResNet-BS.bestjsrt4500.h5\") \n",
    "print(\"Loaded model from disk\")\n",
    "resnet_bs.summary()\n",
    "\n",
    "for f in source:\n",
    "    img = Image.open(f)\n",
    "    img_name = f.split(os.sep)[-1]\n",
    "    \n",
    "    #preprocess the image\n",
    "    img = img.resize((256,256))\n",
    "    x = image.img_to_array(img)\n",
    "    x = x.astype('float32') / 255\n",
    "    x1 = np.expand_dims(x, axis=0)\n",
    "    \n",
    "    #predict on the image\n",
    "    pred = resnet_bs.predict(x1)\n",
    "    test_img = np.reshape(pred, (256,256,1)) \n",
    "    imageio.imwrite(\"C:/Users/result/{}.png\".format(img_name[:-4]), test_img)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Histogram plots and similarity assessment:\n",
    "We plotted the histograms of the ground truth and the predicted bone-suppressed image using the proposed models to observe their tonal distributions. We used various metrics including correlation, intersection, Chi-Squared distance, Bhattacharyya distance, and Earth moverâ€™s distance to compare these histograms and provide a measure of similarity. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#plot multiple histograms of the same image predicted by multiple models\n",
    "import cv2\n",
    "img_g = cv2.imread('image027_target.png',0) #ground truth\n",
    "img_resnetbs = cv2.imread('image027_pred_resnet_bs.png',0) #predicted\n",
    "img_aebs = cv2.imread('image027_pred_ae_bs.png',0)\n",
    "img_cnnbs = cv2.imread('image027_pred_cnn_bs.png',0)\n",
    "img_rlbs = cv2.imread('image027_pred_rl_bs.png',0)\n",
    "\n",
    "# Calculate histogram of each of these\n",
    "hist_g = cv2.calcHist([img_g],[0],None,[256],[0,256])\n",
    "hist_resnetbs = cv2.calcHist([img_resnetbs],[0],None,[256],[0,256])\n",
    "hist_aebs = cv2.calcHist([img_aebs],[0],None,[256],[0,256])\n",
    "hist_cnnbs = cv2.calcHist([img_cnnbs],[0],None,[256],[0,256])\n",
    "hist_rlbs = cv2.calcHist([img_rlbs],[0],None,[256],[0,256])\n",
    "\n",
    "plt.plot(hist_aebs, label='AE-BS', color='brown')\n",
    "plt.plot(hist_cnnbs, label='CNN-BS', color='fuchsia')\n",
    "plt.plot(hist_rlbs, label='RL-BS', color='blue')\n",
    "plt.plot(hist_resnetbs, label='ResNet-BS', color='green')\n",
    "plt.plot(hist_g, label='Ground truth', color='red')\n",
    "plt.xticks(fontsize=12)\n",
    "plt.yticks(fontsize=12)\n",
    "plt.title('Mean', fontsize=12)\n",
    "plt.xlabel(\"Value\", fontsize=12)\n",
    "plt.ylabel(\"Frequency\", fontsize=12)\n",
    "plt.xlim([0,256])\n",
    "plt.ylim([0,800]) # vary this based on your data\n",
    "plt.legend(loc='upper left', prop={\"size\":12})\n",
    "plt.savefig(\"compare_hist.png\", dpi=400)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#compute similarity of a pair of histograms\n",
    "# here we compare the similarity between the histogram of the ground truth image and that \n",
    "#predicted by ResNet-BS model. Repeat the process for other pairs.\n",
    "\n",
    "img_g = cv2.imread('image027_target.png',0) #ground truth\n",
    "img_resnetbs = cv2.imread('image027_pred_resnet_bs.png',0) #predicted\n",
    "\n",
    "# Calculate histogram of each of these\n",
    "hist_g = cv2.calcHist([img_g],[0],None,[256],[0,256])\n",
    "hist_resnetbs = cv2.calcHist([img_resnetbs],[0],None,[256],[0,256])\n",
    "\n",
    "#normalize the histograms\n",
    "hist_g1 = cv2.normalize(hist_g, hist_g).flatten()\n",
    "hist_resnetbs1 = cv2.normalize(hist_resnetbs, hist_resnetbs).flatten()\n",
    "\n",
    "#Using the OpenCV cv2.compareHist function: cv2.compareHist(H1, H2, method)\n",
    "'''\n",
    "cv2.HISTCMP_CORREL: Computes the correlation between the two histograms.\n",
    "cv2.HISTCMP_CHISQR: Applies the Chi-Squared distance to the histograms.\n",
    "cv2.HISTCMP_INTERSECT: Calculates the intersection between two histograms.\n",
    "cv2.HISTCMP_BHATTACHARYYA: Bhattacharyya distance, used to measure the â€œoverlapâ€ between the two histograms.\n",
    "\n",
    "'''\n",
    "correlation = cv2.compareHist(hist_g1, hist_resnetbs1, cv2.HISTCMP_CORREL)\n",
    "print(correlation)\n",
    "chi_square = cv2.compareHist(hist_g1, hist_resnetbs1, cv2.HISTCMP_CHISQR)\n",
    "print(chi_square)\n",
    "intersection = cv2.compareHist(hist_g1, hist_resnetbs1, cv2.HISTCMP_INTERSECT)\n",
    "print(intersection)\n",
    "bhattacharyya_dist = cv2.compareHist(hist_g1, hist_resnetbs1, cv2.HISTCMP_BHATTACHARYYA)\n",
    "print(bhattacharyya_dist)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
